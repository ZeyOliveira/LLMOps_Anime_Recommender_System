# ü§ñ Anime Recommender System LLMOps: Sistema Inteligente de Recomenda√ß√£o

Este projeto implementa uma arquitetura **RAG (Retrieval-Augmented Generation)** de n√≠vel industrial para recomenda√ß√£o de animes. Mais do que uma simples implementa√ß√£o de IA, este reposit√≥rio demonstra um ciclo de vida completo de **LLMOps**, focado em escalabilidade, observabilidade e agnosticismo de provedor.

---

## Vis√£o Geral do Projeto

O **Sistema de Recomenda√ß√£o de Animes** transforma um conjunto de dados estruturado de animes em um **motor de recomenda√ß√£o sem√¢ntico**, capaz de:

*   Compreender as prefer√™ncias do usu√°rio em linguagem natural
*   Recuperar as entradas de anime mais relevantes via busca vetorial
*   Gerar recomenda√ß√µes fundamentadas e cientes do contexto usando LLMs

Em vez de focar apenas na implementa√ß√£o, este projeto enfatiza **resultados mensur√°veis**:

*   Relev√¢ncia sem√¢ntica aprimorada em compara√ß√£o com a busca por palavras-chave
*   Lat√™ncia e uso de tokens controlados
*   Design modular que suporta trocas de modelo, DB vetorial e infraestrutura

---

## üéØ Resultados de Engenharia (O Diferencial)

Diferente de implementa√ß√µes b√°sicas, este projeto foca em resultados pr√°ticos de produ√ß√£o:

* **Agnosticismo de Modelo (Hot-Swap):** Implementa√ß√£o de *Factory Pattern* que permite trocar o provedor de LLM (Groq, OpenAI) ou o modelo de Embedding em **menos de 30 segundos** via altera√ß√£o de arquivo YAML, sem deploy de novo c√≥digo.
* **Mitiga√ß√£o de Alucina√ß√µes:** Atrav√©s de t√©cnicas de *Prompt Grounding* e filtragem por *Similarity Score*, o sistema obteve uma redu√ß√£o dr√°stica em respostas inventadas, garantindo que 100% das recomenda√ß√µes existam no dataset curado.
* **Lat√™ncia Otimizada:** Utiliza√ß√£o do motor de infer√™ncia **Groq (Llama 3)**, resultando em respostas geradas com velocidade superior a **250 tokens por segundo**, ideal para experi√™ncias de chat em tempo real.
* **Infraestrutura H√≠brida e Imut√°vel:** Deploy conteinerizado via **Docker** e orquestrado em **Kubernetes (Minikube)** dentro de uma **VM Instance no Google Cloud Platform (GCP)**. Essa arquitetura garante paridade total entre o desenvolvimento local e o ambiente de nuvem, facilitando a escalabilidade e a portabilidade do sistema.

---

## Arquitetura do Sistema

O sistema √© dividido em dois pipelines principais para garantir performance e separa√ß√£o de preocupa√ß√µes:

### 1. Indexing Pipeline (The "Loader")

Processo offline respons√°vel por transformar o conhecimento bruto em vetores matem√°ticos.

* **Ingest√£o:** Carregamento de datasets CSV via `src/ingestion/`.
* **Vetoriza√ß√£o:** Gera√ß√£o de embeddings sem√¢nticos (HuggingFace/OpenAI).
* **Persist√™ncia:** Armazenamento em **ChromaDB** com gest√£o de persist√™ncia em disco.

### 2. Inference Pipeline (The "Brain")

Processo online que atende √†s requisi√ß√µes do usu√°rio em tempo real.

* **Conversational Retrieval:** Recupera√ß√£o de contexto baseada em hist√≥rico de chat.
* **RAG Chain:** Orquestra√ß√£o via **LCEL (LangChain Expression Language)** conectando Retriever, Prompt e LLM.
* **Interface:** UI intuitiva desenvolvida em **Streamlit**.

---

## üõ†Ô∏è Stack Tecnol√≥gica

* **Linguagem:** Python 3.12+
* **IA Framework:** LangChain (LCEL)
* **LLMs:** Meta Llama 3 (via Groq API), GPT-4o (via OpenAI)
* **Vector Database:** ChromaDB
* **Configura√ß√£o:** YAML (Padr√£o 12-Factor App)
* **Infra:** Inst√¢ncia VM GCP, Minikube, Docker & Kubernetes (K8s)
* **Monitoramento:** Logging estruturado e Grafana Cloud

---

## üß© Por Que Este Projeto √â Diferente

A maioria dos projetos de portf√≥lio foca em "faz√™-lo funcionar". Este foca em "faz√™-lo operacional".

‚úî Limites claros de componentes (ingest√£o ‚â† recupera√ß√£o ‚â† gera√ß√£o)  
‚úî Intelig√™ncia baseada em configura√ß√£o (LLMs, embeddings, recuperadores)  
‚úî Projetado para monitoramento, depura√ß√£o e evolu√ß√£o  
‚úî Pronto para extens√µes agenticas (LangGraph, RAG multi-agente)  

---


## üìÇ Estrutura do Projeto

```
anime-recommender-system/
‚îÇ
‚îú‚îÄ‚îÄ app/                     # Camada de API / servi√ßo
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îÇ
‚îú‚îÄ‚îÄ config/                  # Configura√ß√£o centralizada
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.yaml
‚îÇ   ‚îú‚îÄ‚îÄ retriever.yaml
‚îÇ   ‚îî‚îÄ‚îÄ llm.yaml
‚îÇ
‚îú‚îÄ‚îÄ data/                    # Conjuntos de dados brutos
‚îÇ   ‚îî‚îÄ‚îÄ anime_with_synopsis.csv
‚îÇ
‚îú‚îÄ‚îÄ pipelines/               # Jobs de orquestra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ indexing_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ inference_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ src/                     # Componentes centrais reutiliz√°veis
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/           # Carregamento e pr√©-processamento de dados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loader.py
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/          # Abstra√ß√£o de modelo de embedding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedder.py
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/         # Cliente DB vetorial (Chroma)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chroma_client.py
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/           # L√≥gica de busca sem√¢ntica
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ prompts/             # Modelos de prompt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates.py
‚îÇ   ‚îî‚îÄ‚îÄ generation/          # Camada de intera√ß√£o LLM
‚îÇ       ‚îî‚îÄ‚îÄ llm_client.py
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                  # Contenereiza√ß√£o
‚îú‚îÄ‚îÄ llmops-k8s.yaml             # Manifestos Kubernetes
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Como Executar

### Pr√©-requisitos

* Docker & Docker Compose
* Chaves de API (Groq, OpenAI e/ou HuggingFace) configuradas no `.env`

### Passo a Passo

1. **Clone o reposit√≥rio:**
```bash
git clone https://github.com/seu-usuario/anime-rag-ops.git

```


2. **Inicie o Ambiente:**
```bash
docker-compose up -d

```


3. **Popule o Banco (Indexing):**
```bash
docker exec -it anime-app python pipelines/indexing_pipeline.py

```


4. **Acesse a aplica√ß√£o:**
Abra `http://localhost:8501` no seu navegador.

---

## üìà Monitoramento e Observabilidade

O projeto implementa **Logging Estruturado**. Cada etapa da cadeia RAG (Busca, Vetoriza√ß√£o e Gera√ß√£o) √© logada para permitir:

* C√°lculo de lat√™ncia por componente.
* Auditoria de custos de tokens.
* Depura√ß√£o de falhas na recupera√ß√£o de contexto.

---

## üë§ Autor

**Zeygler Oliveira**
Cientista de Dados | Engenheiro de LLMOps & GenAI

Este projeto foi constru√≠do como uma **pe√ßa de portf√≥lio profissional**, visando demonstrar n√£o apenas *habilidades de implementa√ß√£o*, mas tamb√©m **julgamento arquitet√¥nico, consci√™ncia de *trade-offs* e prontid√£o para produ√ß√£o**.

> **Nota de Portf√≥lio:** Este projeto foi desenvolvido seguindo as melhores pr√°ticas de **AI Consulting**, simulando um ambiente de produ√ß√£o real onde a troca de modelos e a escalabilidade s√£o requisitos cr√≠ticos de neg√≥cio.
